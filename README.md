# Gaze Tracker Model

TODO: Add description, demo

## Installation

1. Clone repository
2. `cd` into project. Note that every script is expected to be run from this directory
3. Install dependencies
  ```sh
  uv sync # or check `pyproject.toml` for requirements if not using uv
  ```

## 1. Dataset Preparation

This project supports the Gaze360 and MPIIFaceGaze datasets. You must download the raw data and then run the provided scripts to preprocess it into the format the model expects.

The expected directory structure is:
```
project-root/
├── data/
│   ├── raw/
│   │   ├── Gaze360/      <-- Unzip Gaze360 here
│   │   └── MPIIFaceGaze/   <-- Unzip MPIIFaceGaze here
│   │   └── MPIIGaze/       <-- Unzip MPIIGaze samples here
│   └── preprocessed/
│       ├── Gaze360/      <-- Preprocessing output
│       └── MPIIFaceGaze/   <-- Preprocessing output
├── configs/
├── scripts/
└── src/
```

Download: Download the datasets and unzip them into `data/raw/Gaze360`, `data/raw/MPIIFaceGaze`

Preprocess:
- Important: Before running, you must edit the root and out_root paths at the top of the `preprocessing/gaze360.py` and `preprocessing/mpiifacegaze.py` scripts to match your local machine's paths.
- Run the script:
  ```sh
  uv run python preprocessing/gaze360.py
  uv run python preprocessing/mpiifacegaze.py
  ```

## Step 2: Training and Evaluation

All workflows are controlled by config files in the `configs/` directory.

### Gaze360 Workflow

#### 1. Configure
Create a YAML file in the `configs/` directory (e.g., `configs/gaze360_train.yaml`).

```yaml
# Dataset
dataset_name: gaze360
data_root: /path/to/Gaze360 # <-- IMPORTANT: Change this path
split: train

# Backbone
backbone: resnet18
pretrained: true

# Params
image_size: 224
batch_size: 16
lr: 0.00001
epochs: 50
alpha: 1.0

# Gaze 360 binning logic
num_bins: 90
angle_range: 360 # Corresponds to [-180, 180]
```

Create another evaluation config YAML file in the `configs/` directory (e.g., `configs/gaze360_eval.yaml`). This config file rewrite the `split` from train config.

```yaml
include: "./gaze360_train.yaml"
split: "test"
```

#### 2. Train
Run the training script with your new config. The script will save the best model (based on validation loss) to `output/.../best.pth`.
```bash
uv run python train.py --config configs/gaze360_train.yaml
```

#### 3. Evaluate
Use the evaluation script to test your trained model on the test set.
```bash
uv run python eval.py --config configs/gaze360_eval.yaml --weights /path/to/your/output/.../best.pth
```

### MPIIFaceGaze Workflow

This dataset uses **leave-one-person-out** cross-validation. The process involves holding out one person for testing and training on all others.

#### 1. Configure
Create a config file (e.g., `configs/mpiifacegaze_train.yaml`). Note that `do_validation` is set to `false`.

```yaml
# Dataset
dataset_name: "mpiigaze"
data_root: "/path/to/MPIIFaceGaze" # <-- IMPORTANT: Change this path
split: "train" # The 'train' split is generated by the prepare_mpii_labels.py script

# Backbone
backbone: "resnet18"
pretrained: true

# Params
epochs: 10
batch_size: 16
lr: 0.00001
alpha: 1.0
image_size: 224

# MPIIFaceGaze uses leave-one-out, so there is no separate validation
# set during the main training run. Validation is done via the eval script.
do_validation: false

# --- Binning Parameters for MPIIFaceGaze ---
# Range: -42 to +42 degrees
num_bins: 28
angle_range: 84 # 42 - (-42)
```

Create another evaluation config YAML file in the `configs/` directory (e.g., `configs/gaze360_eval.yaml`). This config file rewrite the `split` from train config.

```yaml
include: "./mpiifacegaze_train.yaml"
split: "__PERSON_ID__" # this will be replaced by the trainer script
```

#### 2. Train and Evaluate
The output of the script is located at `output/mpii_loocv_results_<TIMESTAMP>`

```bash
uv run mpii_loocv/run.sh
```

## Additional Functionality

### Live Demo
Run a live demo using your webcam. The script uses MediaPipe for face detection.
```bash
uv run python demo.py --config /path/to/your/config.yaml --weights /path/to/your/best.pth
```

### MobileOne Model Optimization
If you train a `MobileOne` backbone, you can fuse its branches to create an optimized model (no accuracy loss) for faster inference.

**1. Reparameterize the Model:**
```bash
# Output flag is optional, by default the script will output the fused model in the same foder as --weights
uv run python reparameterize_mobileone.py --config /path/to/mobileone_training_config.yaml --weights /path/to/best.pth --output /path/to/best_fused.pth
```

**2. Evaluate the Fused Model:**
Use the `--fused` flag to tell the evaluation script to build the model in its inference-time configuration.
```bash
uv run python eval.py --config /path/to/mobileone_training_config.yaml --weights /path/to/best_fused.pth --fused
```

## Misc

### Available Backbones

- resnet18
- resnet34
- resnet50
- mobileone_s0
- mobileone_s1
- mobileone_s2
- mobileone_s3
- mobileone_s4
- lowformer_b0
- lowformer_b1
- lowformer_b15
- lowformer_b2
- lowformer_b3
